{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>Welcome to this searchable AI learning resource!</p> <p>This handy space has a variety of introductory, intermediate and more advanced content suitable for anyone and everyone regardless of where you are in your AI learning journey. The AI space is vast and overwhelming at times, especially when just starting out. </p> <p>This document has a search function if your on the hunt for something specific (in the navbar). If you are fresh to the AI space then head over to the 'Introduction' tab where you can find some introductory content to get things rolling.</p>"},{"location":"#about-this-content","title":"About this Content","text":"<p>There is a broad range of content covered in this resource broken into separate sections that can be comfortably consumed on their own in a short sitting. This resource is focused around AI in health but is useful for general learning also. There are both technical and non-technical focused sections (they tend to be either or) with references to source material as well as many links to video based alternatives (where available) if reading isn't your thing. </p> <p>We have also tried to include further reading, additional examples and links to more advanced material if you want to go further and learn more on your own. </p> <p>If you've arrived here just for the basics and a general overview the 'Introduction' section should be enough to get started, although we do encourage everyone to try and work down through the sections in order to 'Ethics, Social License &amp; Governance' at their own pace. </p> <p>The remaining sections are the ones with a more technical focus, so unless your keen to learn about how machine learning works and like a healthy dose of stats don't feel like your missing out. The 'History of AI' being the exception to this, which is great for understanding the development of this not-so-new technology to better contextualize the present (we understand if history isn't your favorite though).</p> <p>Remember learning never ends, especially with technology developing as rapidly as it does, don't feel like you need to learn it all at once. Come back as interest or situation demands and make use of this as a place to start!</p>"},{"location":"#section-descriptions","title":"Section Descriptions","text":"<ul> <li> <p>Introduction:</p> <p>A primer and general overview to get you started in your AI learning journey that should cover all the main points you should be familiar with.</p> </li> <li> <p>Ethics, Social License &amp; Governance:</p> <p>Ethical considerations in the application of AI technologies, the preservation of privacy, consent, data sovereignty and governance practices. </p> </li> <li> <p>Risk Management:</p> <p>Frameworks for considering and quantifying the risks of AI technologies.</p> </li> <li> <p>Machine Learning:</p> <p>Different categories machine learning techniques, algorithms and related concepts.</p> </li> <li> <p>Deep Learning:</p> <p>Neural networks, generative AI, computer vision and some key concepts related to their function.</p> </li> <li> <p>Modelling:</p> <p>How data can be used to represent things in vector space, sampling, normalization and training models. </p> </li> <li> <p>Model Metrics:</p> <p>Foundational statistical metrics used to evaluate model performance, useful when reading research papers related to the effectiveness of applied AI systems.</p> </li> </ul>"},{"location":"#naiaeag","title":"NAIAEAG","text":"<p>National Artificial Intelligence and Algorithm Expert Advisory Group </p> <p>NAIAEAG advice all Te Whatu Ora employees and contractors to NOT\u200b:</p> <ul> <li> <p>\u201cEnter any personal, confidential or sensitive patient or organizational data into Large language models &amp; Generative AI tools\u201d\u200b</p> </li> <li> <p>\u201cuse large language models &amp; generative AI tools for any clinical decisions, any personalized patient-related documentation or for personalised advice to patients\u201d</p> </li> </ul> <p>The NAIAEAG are committed to continually updating their advice on the use of Large Language models and Generative AI tools. \u200b\u200b</p> <p>To contact NAIAEAG there is a form available for TWO employees to fill out to tell the advisory group what they are using AI for or what they hope to use AI for. \u200b</p> <p>Te Whatu Ora official Advice for LLM and Generative AI use</p>"},{"location":"deep_learning/","title":"Deep Learning","text":"<p>Deep learning is application of multi-layered neural networks (deep neural networks). This is based on the idea of trying to simulate the structure of the human brain (in a very abstract way) by creating interconnected neurons arranged as virtual layers inside of a computer. Deep neural networks operate on principles of mathematical and statistical optimization rather than truly mimicking brain functions.\u200b\u200b</p> <p>Each neuron in layer receives input, processes it and then passes it into the neurons in the next layer. These multi-layer networks can train on datasets that enable them to learn complex patterns and relationships. Neural networks can be applied to many different situations such as regression, classification and famously part of generative AI such as ChatGPT.\u200b\u200b</p> <p>To better understand how neural networks work we will look at a perceptron which is a classic example of a single neuron, how these fit into a wider neural network and how learning takes place during the training process.</p>"},{"location":"deep_learning/#perceptron","title":"Perceptron","text":"<p>This perceptron consists of inputs, weights, bias (which is added to the weighted sum),  an activation function and an output.</p> <p>\u200b</p> <ul> <li> <p>Inputs - If the neuron is in the first layer, the inputs would consist of the input data. In most cases this is the feature vector of an instance (predictor values) from our dataset. If the neuron is in any other layer, the inputs are the outputs of neurons from the preceding layer.\u200b\u200b</p> </li> <li> <p>Weights - Each input is paired with a weight. These weights are how a neuron learns during training , they are modified and fine-tuned over time. Each input is multiplied with its corresponding weight and then the results are summed together before being passed onto the activation function (this number is called a weighted sum).</p> </li> <li> <p>Activation Function - Activation functions are pivotal in neural networks, determining whether a neuron should be activated based on the weighted sum of its inputs. These functions can vary, chosen based on the specific application and characteristics desired in the model. A common example is the sigmoid function, which maps any input value to a range between 0 and 1, making it useful for models where the output is probabilistic or binary. \u200b</p> </li> <li> <p>Bias - is a critically important element of the perceptron and is altered during training. The bias is a single number that is added to the weighted sum before it is passed to the activation function. It acts as a modifier to the threshold of the activation function, effectively moving the function to the left or right on the x-axis, allowing the function to be optimized to the task at hand.</p> </li> </ul>"},{"location":"deep_learning/#neural-network","title":"Neural Network","text":"<p>Neural networks consist of layered neurons, such as perceptron's. The input layer receives the data, with each neuron assigned different weights and biases. Typically, all neurons in a layer share the same activation function.</p> <p></p> <p>Hidden layers connect the input to the output layer, processing the prior layer's neuron outputs. A fully connected network, where each neuron receives outputs from all neurons in the previous layer, is common but not universal.</p> <p>The output layer's activation function depends on the task: a sigmoid function might output probabilities for classification, while a linear function could directly output values for regression.</p> <p>Learning occurs through operations like backpropagation, where after a prediction (forward pass), the network adjusts its weights and biases based on the prediction error, often measured by mean squared error (MSE). These adjustments can be made per prediction (online learning) or in batches.</p> <p>Here is a approachable video of how backpropagation works, allowing these networks to 'learn' by correcting their internal weights - Backpropagation</p> <p>Training involves multiple epochs, where one epoch represents one full pass through the dataset. The scale of the network, volume of data, and extent of weight updates contribute to the computational intensity of neural network training. The limits of training scale are set by the quality of data, computational resources, and time available.</p> <p>A great video explanation of a neural network - Hand written digit classifier</p>"},{"location":"deep_learning/#generative-ai","title":"Generative AI","text":"<p>Generative AI is a sophisticated application of deep learning that extends beyond data analysis to data creation. At its core, it involves training neural networks to replicate and generate patterns found within a dataset. A prime example of generative AI in action is ChatGPT, which is trained on a diverse dataset comprising vast amounts of text from the internet.</p> <p>The training of ChatGPT utilizes a type of neural network called a Transformer. This network architecture is adept at handling sequences, making it particularly suitable for processing and generating text.</p> <p>A handy video walkthrough of a transformer - Transformer explanation</p> <p></p> <p>When ChatGPT processes input text, it breaks it down into 'tokens'. These tokens are then converted into vectors in a multi-dimensional space using a 'word embedding table' that has been trained to keep 'similar' words close to one another. An 'attention mechanism' then weighs the importance of words in context, allowing the model to focus on the relevant parts of the text.</p> <p>ChatGPT needs to be trained to perform these tasks effectively. During its training phase, ChatGPT is presented with text sequences and learns to predict subsequent words based on the preceding context. This learning process resembles a sophisticated form of pattern recognition, where the model learns the likelihood of word sequences. It is a form of 'Self-Supervised Learning'. For example, given the text \"The cat sat on the...\", the model predicts the next word (\"mat\") using the preceding text as context. The \"label\" in this case is the next word in the sequence, which is derived from the text itself, rather than being an external annotation.</p> <p>A unique aspect of ChatGPT's training involves the integration of human feedback. Human labelers play a crucial role by evaluating the generated text for relevance, coherence, and accuracy. This feedback is used to fine-tune the model through a process known as Reinforcement Learning from Human Feedback (RLHF). This process guides the model towards generating responses that not only make logical sense but also align more closely with human expectations and nuances in language use.</p> <p>This iterative training, coupled with feedback from human evaluators, contributes to ChatGPT's development in producing text. The involvement of human labelers in the training of GPT models provides an additional layer of refinement, aiming to improve the model's understanding of human-like text generation. </p> <p>However, while this approach seeks to enhance the model's ability to produce text that may seem coherent and contextually relevant, it's important to acknowledge the inherent challenges and limitations in fully capturing the nuances of human communication through AI. The quality of output can vary, and the model may still generate nonsensical or irrelevant text, especially when dealing with complex topics or nuanced prompts.</p> <p>It's crucial to understand that generative AI, like ChatGPT operate on a statistical basis, analyzing patterns in the data it has been trained on to generate new content. This means that while chatGPT can produce text that is often plausible and reads as if it were written by a human, it is not inherently factual or rigorous. \u200b\u200b</p> <p>The model makes predictions based on the likelihood of word sequences without an understanding of truth or the ability to verify the accuracy of the information it generates. This statistical approach to generating text underscores the importance of treating generative AI's output with scrutiny, especially in contexts where factual accuracy and reliability are paramount.</p> <p>This is not all, though. What about those smart-looking picture makers? DALL-E, Midjourney, Stable Diffusion, Leonardo.AI, Ideogram, and so on? Actually, the way they work is fairly similar to what we\u2019ve just described, with some wrinkles.</p> <p>The text prompt that you put in is broken up into tokens\u2014but instead of mapping to text, the tokens are mapped into a vector space that represents encoded images. This information is then used by an image decoder to build your image.</p> <p>With all of the above, I\u2019ve skipped over something very important. Training. For example, with DALL-E, we need to train our model so that it correctly links a picture of (say) the token corresponding to \u2018hippopotamus\u2019 with pictures of hippopotami. This is done by comparing the image to N pictures accurately labeled to represent a hippo\u2014and N2 pictures of \u201cnot-hippos\u201d for reference purposes. This is a general theme with these AI models. They are hungry for large amounts of information. They are correspondingly energy hungry. Think \u2018climate change\u2019.</p> <p>When you meet the term generative AI, this refers to \u2018AI\u2019 based on the above approach. The same principles can be used to generate text, computer code, images, music, and even videos.</p>"},{"location":"deep_learning/#computer-vision","title":"Computer Vision","text":"<p>Computer vision is perhaps the most exciting area of AI from a clinical perspective. Visual data lends itself very well to being processed by neural networks, particularly those utilizing convolutions. A convolution is a mathematical operation that applies a filter (or kernel) to an image. This filter slides over the image, performing a dot product between the filter and the local regions of the image, which highlights specific features such as edges, textures, or patterns. These extracted features are often imperceptible to a human viewing the original image.</p> <p></p> <p>By stacking multiple layers of convolutions, neural networks can build a hierarchy of features, allowing them to recognize complex structures within the image. This process enables the network to progressively learn more abstract and high-level representations of the visual data.</p> <p></p> <p>In practical applications, convolutional neural networks (CNNs) have many actives applications in the field of radiology. For example, they are being actively researched and applied in areas such as fracture detection, CT interpretation, and the diagnosis of diabetic retinopathy. The ability of CNNs to process and analyze visual data with high accuracy holds great promise for improving diagnostic processes and patient outcomes in the medical field.</p> <p>A video based breakdown on how those convolutions works - Convolutions</p> <p>Computer vision systems trained using deep learning techniques can assist clinicians by providing preliminary analysis and highlighting areas of interest in medical images, thus enhancing the efficiency and accuracy of diagnoses.</p>"},{"location":"deep_learning/#further-learning","title":"Further Learning","text":"<p>Gradient descent is a crucially important concept, and method that neural networks and other machine learning algorithms use to minimize their errors during training - Gradient Descent</p> <p>As impressive as large models have been in recent memory, state-of-the art applications for AI systems are increasingly heading towards compound AI systems comprised of multiple components working together instead of single large models:</p> <ul> <li> <p>Compund AI Systems</p> </li> <li> <p>Large language model abstractions</p> </li> </ul>"},{"location":"ethics_social_license_governance/","title":"Social license, Governance and Ethics","text":""},{"location":"ethics_social_license_governance/#social-license","title":"Social License","text":"<p>The term social licensing refers to societal acceptance earned through reliable and ethical practices.  In reference to AI, it\u2019s the difference between possessing the legal right to use AI and earning societal acceptance to use AI. Social licenses are won not bought.</p> <p>Healthcare systems worldwide face immense pressure to achieve the quadruple aim: improving population health, enhancing patients' experiences of care, boosting caregiver satisfaction, and reducing the rising costs of care. AI has emerged as a transformative tool with the potential to revolutionize healthcare. However, for AI to be effective, the public must trust that AI algorithms are accountable, fair, and transparent. The current approach often emphasizes identifying healthcare problems and applying AI solutions without fully considering clinical workflows, user needs, trust, safety, and ethical implications.</p> <p>Historical AI failures highlight the importance of responsible AI use and the need for a social license. Microsoft's TayTweets, an AI chatbot, quickly turned into a PR disaster due to its inability to handle malicious inputs, resulting in racist and offensive outputs. Amazon's AI hiring tool, intended to streamline recruitment, was found to be biased against women because it was trained on historical data predominantly featuring male candidates. These examples underscore the necessity of designing AI systems with ethical considerations and robust oversight to prevent such failures.</p> <p>Modern AI is a relatively new discipline, and public understanding of its functions is limited. It is crucial to demystify AI and its role in healthcare, demonstrating its benefits, justifying its use in projects, and ensuring data protection. Effective data governance is the foundation for building trust and accountability in using AI in healthcare. Clear and transparent data governance practices reassure the public that their data is being handled responsibly and ethically, which is essential for gaining a social license.</p> <p>To implement AI effectively in healthcare, there needs to be a clear demonstration of responsible AI practices. Companies must be transparent about their algorithms' workings and outcomes, ensuring that AI applications are perceived as fair and transparent. AI systems must be designed to avoid demographic biases and provide consistent and equitable results. Additionally, businesses must communicate the benefits and potential downsides of AI openly, educating all stakeholders about the technology's implications.</p> <p>Gaining a social license for AI in healthcare involves more than just meeting regulatory requirements. It requires ongoing dialogue with stakeholders, including patients, healthcare providers, and the public, to address their concerns and expectations. Companies must prioritize human oversight, develop mechanisms to handle exceptions, and be transparent about AI's decision-making processes. By doing so, they can earn the trust and acceptance needed to integrate AI into healthcare systems effectively.</p> <p>In conclusion, achieving a social license for AI in healthcare is crucial for its successful implementation. By ensuring responsible AI practices, transparent data governance, and ongoing stakeholder engagement, healthcare systems can harness the transformative potential of AI while maintaining public trust and confidence.</p> <p></p>"},{"location":"ethics_social_license_governance/#data-governance","title":"Data Governance","text":"<p>Data governance is the structured process of managing the availability, usability, integrity, and security of data within government and enterprise systems. It involves setting internal standards and policies to control data usage, ensuring data consistency, and preventing misuse. Effective data governance is crucial for organizations to comply with expanding data privacy regulations and to optimize operations through reliable data analytics, driving informed business decision-making. In healthcare, this means ensuring that patient data is accurate, secure, and accessible for improving patient care and operational efficiency.</p> <p>Without effective data governance, inconsistencies in patient data across different systems can lead to operational challenges, affecting the accuracy of business intelligence, reporting, and data-driven decision-making in healthcare. Poor data governance can also hinder compliance with data privacy laws, posing risks to patient confidentiality and organizational integrity. For healthcare organizations, robust data governance is essential to ensure data integrity, protect patient privacy, and maintain trust.</p> <p>Modern AI is a relatively new discipline, and social understanding of its function is limited. It's important to demystify the term AI and its role in healthcare moving forward, to demonstrate its benefits, justify its existence within projects and communicate how data will be protected. </p> <p>For the healthcare system to implement AI functions into their day-to-day operations, there needs to be a clear demonstration that good data governance practices are in place. \u200bEffective data governance is the foundation of forging a relationship of trust and accountability around the use of AI in healthcare.</p>"},{"location":"ethics_social_license_governance/#nzs-data-information-management-principles","title":"NZ's data &amp; information management principles","text":"<ul> <li> <p>Open: Data held by the New Zealand government needs to be open to the public.\u200b</p> </li> <li> <p>Protected: Personal, confidential and classified data is protected\u200b\u200b.</p> </li> <li> <p>Readily available: Open data and information is released proactively. Data is discoverable and accessible\u200b.\u200b</p> </li> <li> <p>Trusted  &amp; authoritative: Data collected supports the purpose for which it was collected. </p> </li> <li> <p>Well managed: Data held/ owned by the government belongs to the NZ public. Should only be collected/ generated for specified purposes\u200b.\u200b</p> </li> <li> <p>Reasonability priced: Use and re-use of government held data is expected to be free\u200b.\u200b</p> </li> <li> <p>Reuseable: data and information released can be discovered, shared and reused over time and through technology changes.</p> </li> </ul>"},{"location":"ethics_social_license_governance/#ethical-considerations-of-ai-in-healthcare","title":"Ethical Considerations of AI in healthcare","text":"<p>There are 3 key ethical consideration for the use of AI in healthcare\u200b:</p> <p>Privacy and data protections: \u200bUtilizing AI in healthcare involves the collection, storage and analysis of sensitive health data (e.g., patient records) and AI analysis of such data accompanies inherent risks. The ethical dilemma is posed around how to balance the privacy of patients and the accuracy of the AI. How much private health information can be programed into an AI algorithm and how is this data going to be protected? </p> <p>Ensuring that health information used in AI systems is protected requires all actors involved in its development and implementation to understand the risks associated with its use, how to mitigate those risks how to legislate its use appropriately. </p> <p>Informed consent and autonomy: There needs to be \"transparent and understandable\" disclosure to patients surrounding how their information is being use and stored.Concerns arise when discussing how the use of AI in patient care interacts with the patients right to informed consent.To what extent do clinicians have to educate patient on the AI algorithms used to reach a particular decisions or diagnoses? Are clinicians themselves educated enough to explain this adequately?</p> <p>Another area of concern is the use of Blackbox algorithms, referring to when AI developers show the outputs of their algorithms but not their internal workings to protect their intellectual property.  Using Blackbox algorithms in healthcare would violate patients right to informed consent as explanations for how decisions were reached or how personal information was used would not be provided \u200b Algorithmic bias and discrimination: Any machine learning algorithm is only as trustworthy as the data is it trained on. An important ethical consideration for AI's use in the healthcare systems is that the algorithm's training data does not reflect the racial and gender-based discrimination presented in society.</p> <p>Marginalised communities' historical exclusion from research and statistics has led to these groups being underrepresented in the algorithms training data. This can exacerbate existing stereotypes if not appropriately accounted for. Using bias data to train AI algorithms will promote further discrimination within the healthcare system.</p>"},{"location":"ethics_social_license_governance/#maori-data-sovereignty-principles-of-maori-data-sovereignty","title":"M\u0101ori Data Sovereignty &amp; Principles of M\u0101ori Data Sovereignty","text":"<p>M\u0101ori Data Sovereignty refers to the inherent rights and interests that M\u0101ori have in relation to the collection, ownership, and application of M\u0101ori data. It encompasses the principles, structures, accountability mechanisms, legal instruments, and policies through which M\u0101ori exercise control over their data. This concept is vital for the protection and promotion of M\u0101ori rights and interests, ensuring that data is used ethically and in ways that enhance the wellbeing of M\u0101ori people, language, and culture.</p> <p>M\u0101ori Data Sovereignty is crucial to New Zealand as it acknowledges and respects the unique relationship between M\u0101ori and their data. This relationship is rooted in the broader context of M\u0101ori rights as Indigenous peoples, including their connections to land, water, and the natural world. Recognized by Te Tiriti o Waitangi (The Treaty of Waitangi) and the United Nations Declaration on the Rights of Indigenous Peoples (UNDRIP), these rights emphasize the need for M\u0101ori to have authority over their data. This governance ensures that data practices support M\u0101ori self-determination, cultural preservation, and community wellbeing, contributing to the overall equity and justice within New Zealand society.</p> <p>Rangatiratanga | Authority</p> <p>1.1 Control: M\u0101ori have the inherent right to control their data and data ecosystems, including creation, collection, access, analysis, interpretation, management, security, dissemination, use, and reuse.</p> <p>1.2 Jurisdiction: Decisions about the storage of M\u0101ori data should enhance control for current and future generations, preferably keeping data within Aotearoa New Zealand.</p> <p>1.3 Self-determination: M\u0101ori have the right to data that empowers sustainable self-determination and effective self-governance.</p> <p>Whakapapa | Relationships</p> <p>2.1 Context: All data has a whakapapa (genealogy). Metadata should provide information about the data's provenance, collection purposes, context, and involved parties.</p> <p>2.2 Data disaggregation: Disaggregating M\u0101ori data increases its relevance for M\u0101ori communities and iwi, using categories that prioritize M\u0101ori needs and aspirations.</p> <p>2.3 Future use: Decisions made today about data can have long-term consequences for future generations of M\u0101ori, emphasizing the need for protective data governance.</p> <p>Whanaungatanga | Obligations</p> <p>3.1 Balancing rights: Individual rights, including privacy, must be balanced with group rights. In some cases, collective M\u0101ori rights will prevail over individual rights.</p> <p>3.2 Accountabilities: Those responsible for M\u0101ori data are accountable to the communities, groups, and individuals from whom the data derives.</p> <p>Kotahitanga | Collective benefit</p> <p>4.1 Benefit: Data ecosystems should enable M\u0101ori to derive individual and collective benefits.</p> <p>4.2 Build capacity: Developing a M\u0101ori workforce is essential for the creation, collection, management, security, governance, and application of data.</p> <p>4.3 Connect: Supporting connections between M\u0101ori and other Indigenous peoples to share strategies, resources, and ideas related to data.</p> <p>Manaakitanga | Reciprocity</p> <p>5.1 Respect: Data collection, use, and interpretation should uphold the dignity of M\u0101ori communities, avoiding analysis that stigmatizes or blames M\u0101ori.</p> <p>5.2 Consent: Free, prior, and informed consent (FPIC) should underpin the collection and use of M\u0101ori data, balanced by robust governance arrangements.</p> <p>Kaitiakitanga | Guardianship</p> <p>6.1 Guardianship: Data storage and transfer should enable M\u0101ori to exercise kaitiakitanga (guardianship) over their data.</p> <p>6.2 Ethics: Tikanga, kawa (protocols), and m\u0101tauranga (knowledge) should guide the protection, access, and use of M\u0101ori data.</p> <p>6.3 Restrictions: M\u0101ori should decide which data is controlled (tapu) or open (noa) access.</p>"},{"location":"ethics_social_license_governance/#2020-privacy-act-13-principles","title":"2020 Privacy Act 13 Principles","text":"<p>The Privacy Act 2020 outlines 13 privacy principles that govern the collection, handling, and use of personal information. These principles are designed to protect individuals' privacy rights and ensure that organizations manage personal data responsibly and transparently.</p> <ol> <li> <p>Purpose of Collection:</p> <ul> <li>Collect personal information only if it is for a lawful purpose and necessary for that purpose.</li> <li>Avoid collecting unnecessary identifying information.</li> </ul> </li> <li> <p>Source of Information:</p> <ul> <li>Generally, collect personal information directly from the individual unless:</li> <li>The person gives permission.</li> <li>It would not prejudice the person's interests.</li> <li>Direct collection would undermine the purpose.</li> <li>Information is from a public source.</li> </ul> </li> <li> <p>Notification:</p> <ul> <li>Inform individuals about:</li> <li>The reason for collection.</li> <li>Who will receive it.</li> <li>Whether it is compulsory or voluntary.</li> <li>Consequences of not providing information.</li> <li>Exceptions exist if notification undermines the collection purpose or is impossible.</li> </ul> </li> <li> <p>Manner of Collection:</p> <ul> <li>Collect information lawfully, fairly, and non-intrusively, especially from children and young people.</li> </ul> </li> <li> <p>Security Safeguards:</p> <ul> <li>Implement safeguards to prevent loss, misuse, or unauthorized disclosure of personal information.</li> <li>Limit employee access to personal information.</li> </ul> </li> <li> <p>Access to Personal Information:</p> <ul> <li>Individuals have the right to access their personal information.</li> <li>Access may be denied for safety, harassment, crime investigation, or privacy reasons.</li> </ul> </li> <li> <p>Correction of Information:</p> <ul> <li>Individuals can request corrections to their information.</li> <li>Attach a statement of correction if the organization disagrees with the correction.</li> </ul> </li> <li> <p>Accuracy of Information:</p> <ul> <li>Ensure personal information is accurate, complete, relevant, up-to-date, and not misleading before use or disclosure.</li> </ul> </li> <li> <p>Retention of Information:</p> <ul> <li>Do not keep personal information longer than necessary.</li> </ul> </li> <li> <p>Use of Information:</p> <ul> <li>Use personal information only for the original purpose or directly related purposes.</li> <li>Obtain permission for other uses or use in limited circumstances.</li> </ul> </li> <li> <p>Disclosure of Information:</p> <ul> <li>Disclose personal information only if:</li> <li>It aligns with the purpose of collection.</li> <li>Authorized by the person.</li> <li>Used anonymously.</li> <li>Necessary to avoid endangering health or safety.</li> <li>Necessary for law maintenance.</li> </ul> </li> <li> <p>Overseas Transfer of Information:</p> <ul> <li>Ensure adequate protection before sending personal information overseas.</li> <li>Obtain express permission if protections are inadequate, unless for law enforcement or safety purposes.</li> </ul> </li> <li> <p>Unique Identifiers:</p> <ul> <li>Use unique identifiers (e.g., IRD or driver\u2019s license numbers) only if necessary for operational functions.</li> <li>Avoid using the same identifier as other organizations.</li> <li>Minimize the risk of misuse, such as identity theft.</li> </ul> </li> </ol>"},{"location":"introduction/","title":"AI 101, first steps on the learning journey!","text":"<p>Equip yourself with a general grasp of AI, machine learning, neural nets, and the questions we should have in mind as AI technology permates our daily live at an increasing pace.</p> <p>What is AI ? That is one of those fun questions that has everyones favourite answer, it depends.  AI can mean alot of different things to many people, it is a constantly evolving field and the way its spoken about is not always consistant. Before we stuck into the weeds of it all, we just want start of by clarifying a few terms and getting orientated around how things fit together.</p>"},{"location":"introduction/#how-things-relate","title":"How things relate","text":"<p>The main terms you'll normally hear thrown around are: Algorithms, Artificial Intelligence, Machine Learning, Deep learning, Predictve AI and Generative AI.</p> <p>First a good 'ol ven diagram on how the relavant fields of study surrounding AI technology:</p> <p></p> <p>The main thing to take from this diagram is that all of these concepts that we will talk about are nested within computer science, draw heavily on statistics and combine techniques from both to find new ways of leveraging the world of data for various purposes. </p> <p>Most of these 'techniques' are be lumped together with the umbrella term 'algorithm':</p> <p></p> <p>The term \"algorithm\" encompasses a broad range of analytical tools, from simpler regression models and decision trees used for predictions and streamlining processes, to more complex systems like neural networks and Bayesian models that utilize machine learning for advanced calculations and predictions, or just a simple set of instructions as depicted above. </p> <p>It's important to note that the risks and benefits of algorithms are not solely determined by their complexity. Even simple algorithms can have significant impacts, positive or negative, depending on their purpose and application. As such, focusing on the potential consequences and impact of algorithms is crucial, rather than solely on technical definitions or specific types.</p> <p>Check out New Zealands algorithim charter for a great 3-pager: New Zealand's Algorithim Charter</p>"},{"location":"introduction/#what-is-the-difference-between-ai-and-algorithims","title":"What is the difference between AI and algorithims ?","text":"<p>While 'algorithm' can be used as a general label, when we talk about AI we generally mean something more precise.</p> <p>The fundamental distinction between AI and traditional algorithms is that they are dynamic. They are not constrained to predetermined steps and boundaries and can cope with unforeseen situations. AI algorithms and systems can overtime, learn new strategies and rules to accomplish tasks.</p> <p>Unlike traditional algorithms, which directly map out the steps to complete a task, AI algorithms focus on the methodology for learning or discovering the steps and rules necessary for task completion.</p> <p>This capability underscores AI as the broader field of computer science dedicated to enabling computers to solve tasks autonomously, without direct human input.</p>"},{"location":"introduction/#ai-v-machine-learning","title":"AI v Machine Learning","text":"<p>These two terms are often confused, conflated and interchanged. They are not the same things.</p> <p>AI is the broad discipline aimed at enabling computers to solve tasks independently, whereas machine learning is its subset, focused on data-driven algorithms and techniques, equipping computers with the ability to learn and make data-informed decisions.</p> <p>This field includes deep learning, which uses multi-layered neural networks for complex data analysis, computer vision, voice recognition, autonomous driving and large language models like ChatGPT through continuous learning. Machine learning drives other tasks such as inference for uncovering explainable relationships in data, and predictive classification for individual risk of disease. </p>"},{"location":"introduction/#deep-learning","title":"Deep Learning","text":"<p>Is is the sub-domain where we find neural nets, generative AI, computer vision and popular tools like ChaptGPT. </p> <p></p> <p>Deep learning is application of multi-layered neural networks (deep neural networks). This is based on the idea of trying to simulate the structure of the human brain (in a very abstract way) by creating interconnected neurons arranged as virtual layers inside of a computer. Deep neural networks operate on principles of mathematical and statistical optimization rather than truly mimicking brain functions.</p> <p>Each neuron in layer receives input, processes it and then passes it into the neurons in the next layer. This carries on until the network produces an output, ('forward pass'). This output is compared to a target or desired result, the network can use the error between the two values to adjust it's layers in response and 'learn' ('back propagation'). These multi-layer networks can train on datasets that enable them to learn complex patterns and relationships. Neural networks can be applied to many different situations such as regression, classification and famously part of generative AI such as ChatGPT.</p>"},{"location":"introduction/#predictive-ai","title":"Predictive AI","text":"<p>Is the more traditional application where past data is utilized to identify anomalies patterns and relationships using a network such as a recurrent neural net. These insights are then used to make predicitve analysis about what may occur in the future. </p>"},{"location":"introduction/#computer-vision","title":"Computer Vision","text":"<p>Computer vision is perhaps the most exciting area of AI from a clinical perspective. Visual data lends itself very well to being processed by neural networks.  These kinds of neural networks utilize components called convolutions that enable them to extract out various characteristics of images, often imperceptible to a human viewing the original image by apply various kinds of filters. </p> <p>Applications in radiology such as fracture detection, CT interpretation and Diabetic retinopathy are just a few example of active areas where Computer Vision is being applied and researched. </p> <p></p>"},{"location":"introduction/#generative-ai","title":"Generative AI","text":"<p>Generative AI is a sophisticated application of deep learning that extends beyond data analysis to data creation. At its core, it involves training neural networks to replicate and innovate on the patterns found within a dataset. A prime example of generative AI in action is ChatGPT, which is trained on a diverse dataset comprising vast amounts of text from the internet.</p> <p>It's crucial to understand that generative AI, like ChatGPT, operate on a statistical basis, analyzing patterns in the data it has been trained on to generate new content. This means that while it can produce text that is often plausible and reads as if it were written by a human, it is not inherently factual or rigorous. The model makes predictions based on the likelihood of word sequences without an understanding of truth or the ability to verify the accuracy of the information it generates.</p>"},{"location":"introduction/#faq","title":"FAQ","text":"<p>Can LLMs actually think? </p> <p>No. Enthusiastic advocates like Sam Altman have said that AIs and people are all \u201cstochastic parrots\u201d, but this is incorrect. Current AIs merely work at the level of association\u2014with a random component. They fail to work at higher levels, specifically they can\u2019t do causal reasoning, and they can\u2019t reliably answer the question \u201cWhat would have happened?\u201d They also struggle with basic things you\u2019d imagine a machine would be good at, like counting objects and checking their own logic. They are however very good at word association on a broad scale\u2014this is explained by how they are built (above). They have no internal model of reality. </p> <p>In Medicine where I work, we are risk-averse. What are the risks of AI?</p> <p>Let\u2019s start with the risks, because you\u2019ve likely heard all of the hype already. That doesn\u2019t mean there\u2019s no role for AI\u2014but it\u2019s nice to start on a firm footing. Here are the problems with LLMs in particular:</p> <ul> <li>Confabulation. Current AIs use broad word association to fill in the gaps, with a random component. As they have no internal model that allows correction, they make stuff up. (This is sometimes called \u2018hallucination\u2019, but they have no brain to hallucinate with). </li> <li>Sycophancy. AIs weight their response based on your input. Effectively, they \u201ctry to please\u201d. This is problematic when it comes to consolidating patient information. </li> <li>Unreliability. If you ask the same question, you won\u2019t necessarily get the same answer. This is problematic when it comes to managing medical information. </li> <li>Bias. The training data set will always have bias\u2014for example, misogyny and bias against minority groups. As this is built in, the AI will overtly or subtly bias everything it produces, accordingly. </li> <li>Privacy concerns loom large with many commercial applications. What is retained and how available it is, is often opaque to all users. With generated texts and images, the issues of plagiarism and violation of copyright are also unresolved. </li> <li>Perhaps the greatest threat is our human fallibility. This is not just trust in the plausible but wrong things an \u2018AI\u2019 might produce for us if we use it in the workplace. Increasingly, bad actors are using AIs for fraudulent purposes, inducing victims to act based on data, \u2018conversations\u2019 or even videos that are faked. </li> </ul> <p>In contrast, the risks of \u201cAI taking over the world\u201d are negligibly small, despite what Elon Musk and Geoffrey Hinton might say.</p> <p>How might AI benefit us?</p> <p>Much has been said about the possible benefits of AI in medicine. These relate to:</p> <ul> <li>Specific tasks like radiological diagnosis, examination of histology, and screening of photographs of the fundus of the eye. </li> <li>Speech synthesis and translation. </li> <li>Consolidation of patient information. </li> <li>Diagnostics and prediction. </li> <li>Making our work easier, including administrative tasks, note-writing. </li> <li>Teaching, including synthesis of cases, and creation of presentations. </li> <li>Producing educational material for patients, explaining conditions and their management, and helping with patient enquiries and trial enrolment.</li> <li>There is now good evidence that in specific circumstances, AI may be useful for the first two tasks\u2014with due caution about translation limitations. For the rest, we still have as-yet-unsolved problems\u2014those already outlined in the previous section.  You might not want a helper that is unreliable, sycophantic, and makes things up on the fly. Especially as it is still very difficult or impossible to get an LLM to explain \u201cwhy it said something\u201d.</li> </ul> <p>Where should I be using large language models (LLMs)?</p> <p>Apart from specific products that have been certified for use in limited tasks, AI simply has too many unresolved issues\u2014those already noted\u2014to be used in clinical work. Particularly concerning are issues related to privacy, bias, and generation of misleading or false information. This may change, but a lot of work is required. We can\u2019t simply extrapolate. </p> <p>Will AI help me with security?</p> <p>For several reasons, AI is a security nightmare. It is unreliable, unpredictable, easily deceived, difficult to analyse forensically, and useful to bad actors in violating security. </p> <p>Will AI take my job? When will we have \u201cArtificial General Intelligence\u201d?</p> <p>Predictions vary. The main issues that need to be overcome before AI surpasses human abilities or even meets them reliably are non-trivial:</p> <ul> <li>These models scale very poorly. As noted, these models require training on vast amounts of data, particularly if the transformer architecture is used. Recent, solid research shows that the idea that you can train up a model \u201csufficiently\u201d and that they will then provide \u201czero shot\u201d performance is wrong. Zero-shot performance means that they can explain and use a completely unfamiliar concept simply by being told about it. For example, they might never have encountered a picture of an ocarina, but will be reliably able to identify it from a terse description. In contrast, these models are wasteful with data, so they require exponentially more data for linear improvements in performance. </li> <li>Confabulation is built in. The AI has no internal model of the real world, so it can\u2019t build on this. Nobody has a solution for confabulation, yet. We may never find one. </li> <li>Getting causal inference and counterfactual reasoning right is hard. We\u2019ve been struggling for 60 years now, and LLMs are not a step in the right direction. We simply don\u2019t yet know how to marry the required logic to LLMs. </li> <li>LLMs are inscrutable\u2014and cannot explain their \u2018reasoning\u2019, which simply emerges from a matrix of coefficients. You can\u2019t explain an internal model that doesn\u2019t exist. </li> <li>These models are unscientific, and unable to do \u201cgood science\u201d. This is perhaps their least understood and most significant limitation.</li> </ul> <p>To end, I will explain a modern view of how science works, and contrast this with the \u201cdata-driven\u201d approach. This is best explained by the following diagram.</p> <p></p> <p>On the left, we have a machine-learning model. Data flows in and is processed, producing predictions. You can see the problems\u2014bias is burnt into the data and into the processes used for extraction. </p> <p>In contrast, modern science starts with real-world problems. We construct explanatory models, which we then test for both internal consistency and real-world validity. A good scientist will try to attack their own model, because they know it is never absolutely true. Data will be used to pursue this goal\u2014and if the model survives (most wont!) it is only considered provisionally true. The process never stops. </p> <p>Science succeeds because it knows it is wrong and lets the data-inform that wrongness; data-driven thinking fails because it makes assumptions of correctness, starting with the belief that data can ever be collected objectively. A derived model can only ever be self-fulfilling. We have work to do! </p>"},{"location":"machine_learning/","title":"Machine Learning","text":"<p>This section will introduce you to the three main categories of machine learning algorithms. There will be several examples, video explainer alternatives and further learning materials for those wanting a deeper technical understanding linked throughout. </p> <p>This section is focused on gaining a general understanding of how these algorithms work. The first part on supervised learning will be of value for most as these kinds of algorithms are pervasive in all sectors. The Unsupervised and Reinforcement learning sections are much less relevant and included for reference and interest. </p> <p></p>"},{"location":"machine_learning/#supervised-learning","title":"Supervised Learning","text":"<p>Fundamentally Supervised learning is the process of learning how to map inputs to outputs based on a collection of examples.</p> <p>Inputs and outputs both come from the features of a dataset. We decide on 'output' feature(s) (a.k.a. target variable(s)) and input feature(s) (a.k.a. predictor(s)).\u200b\u200b</p> <p>The computer uses the predictors to make its best guess of a corresponding target value, then checks its answer with the actual answer (for that instance) and corrects its mechanism for predicting the answer as it goes. \u200b\u200b</p> <p>When done well the computer can learn general rules and relationships between the predictors and the targets that enable it accurately make predictions with new unseen data and/or discover important relationships between features.</p> <p></p>"},{"location":"machine_learning/#classification-regression","title":"Classification &amp; Regression","text":"<p>There are two different forms in which supervised learning takes, classification and regression.\u200b</p> <p>Classification is used to classify instances into two or more distinct groups. The target outputs would be the correct group label for each instance. e.g., based on a person's medical information would they be classified at low, medium or high risk of coronary heart disease. \u200b</p> <p>Regression is concerned with producing a numeric output. Target outputs would be a correct number or measurement. e.g., based on a person\u2019s indirect measurements, what is their current hemoglobin level.\u200b\u200b</p> <p>To see predictive regression as an interactive example, NZRisk is a model made to estimates post-op mortality risk for patients in NZ based on various attributes, looking at 30 days, 1 and 2 years post-op: nzRISK Surgical Mortality Model</p> <p></p>"},{"location":"machine_learning/#inference-and-prediction","title":"Inference and Prediction","text":"<p>Generally, there are two main intentions or goals behind developing a supervised learning model that can affect what techniques are used and other considerations or requirements for the model to meet.\u200b</p> <p>Inference is about gaining insight and understanding about the predictor's relationship and influence over the target variable(s). This is dependent on the model being interpretable.\u200b</p> <p>Model interpretability is the degree to which a human can discern how a model arrived at a given output. Different ML techniques have varying degrees of interpretability, from white box model such as linear regression where you know exactly how it produced an answer, to black box models where it is nearly impossible to determine the means such as neural networks. It is a very active area of research and development to create techniques to help solve the interpretability problem and there are several toolkits available for certain models that helps address this to a degree (e.g., SHAP and LIME for gradient boosted trees).\u200b</p> <p>Prediction focuses on how reliably and accurately a model can classify entities or make estimations. In this context, model interpretability often takes a back seat, and we prioritize techniques that offer the best predictive performance. Essentially, we're most concerned with how good the predictions are, rather than understanding the exact pathway a model takes to arrive at them. While having both high accuracy and interpretability is always preferable, achieving this balance is not always possible.</p>"},{"location":"machine_learning/#supervised-learning-examples","title":"Supervised Learning Examples","text":"<p>Regression Examples:</p> <p>The 'hello world' of machine learning -  Linear Regression</p> <p>A more useful technique when we want to account for multiple variables - Multiple Regression </p> <p>Classification Examples:</p> <p>Yes the name is a bit confusing - Logistic Regression</p> <p>Classic example of an easily explainable algorithm - Decision Trees</p>"},{"location":"machine_learning/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Unsupervised learning deals with data that only consists of inputs without any pre-assigned labels or target values. Unlike supervised learning, where models learn from labeled examples, unsupervised learning involves discovering the underlying structure or patterns within the data on its own. \u200b\u200b</p> <p>This type of learning does not involve direct instruction but rather enables the model to identify relationships and patterns through exploration. \u200b</p> <p>Common techniques in unsupervised learning are clustering, dimensionality reduction and association rule learning.</p> <p></p>"},{"location":"machine_learning/#clustering","title":"Clustering","text":"<p>Clustering (Cluster Analysis) is used to discover groupings of data, by asking the computer to cluster data into x-clusters or dynamically discover the number of clusters itself. \u200b    \u200b</p> <p>There are four main types of clustering:\u200b</p> <pre><code>\u200b\n</code></pre> <ul> <li>Connectivity (Hierarchical) clustering, groups data by creating a dendrogram, a tree-like structure, based on the proximity of data points, using distance measures such as Euclidean or Manhattan distance. This method can be divisive, starting with all data in one cluster and dividing it into smaller clusters, or agglomerative, where each data point starts in its own cluster and is progressively merged into larger clusters. The result is a hierarchy of clusters that can be divided at different levels to obtain a granular or broad grouping.\u200b</li> </ul> <p></p> <ul> <li>Centroid (Partition) clustering, such as K-Means, assigns data points to clusters based on their closeness to the center (centroid) of the cluster. This method requires pre-defining the number of clusters and iteratively adjusts the centroids and cluster assignments to minimize variance within each cluster.</li> <li>Density clustering, focuses on areas of high data point concentration to form clusters, separated by areas of low density. It's capable of identifying clusters of arbitrary shapes, is effective at handling noise and outliers. The core concept is that each point in a cluster must have x number of fellow cluster members within y radius of itself.</li> <li>Distribution clustering, groups data points based on the likelihood of belonging to the same statistical distribution, such as Gaussian or Binomial. This approach assumes the data points in a cluster follow the same distribution, with each cluster characterized by parameters of that distribution. It's flexible in the shapes and sizes of clusters it can detect when its assumptions are met.</li> </ul> <p></p>"},{"location":"machine_learning/#clustering-examples","title":"Clustering Examples","text":"<p>The 'hello world' of clustering, a centroid example  - K-Means</p> <p>One of the most effective and popular clustering methods, a density example - DBSCAN</p> <p>The odd one out of these techniques - hierarchical Cluster</p> <p>A Distribution example, can be very mathy - Gaussian Mixture Models</p>"},{"location":"machine_learning/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction techniques enable us to systemically reduce the number of features under consideration, by pruning the less informative dimensions or combining them into more meaningful ones.  This can help mitigate the risk of overfitting, reduce computational load, and make visualization of high-dimension data possible. \u200b</p> <p>There are two main categories of Dimensionality reduction:\u200b\u200b</p> <ul> <li> <p>Feature Selection: Selects the most informative and significant features from the original dataset. Feature selection methods include filter methods, wrapper methods, and embedded methods, each differing in how they assess the importance of the features, but all aiming to retain only those that contribute most to the predictive power of the model. *Most are not unsupervised learning.\u200b</p> </li> <li> <p>Feature Extraction: Involves creating new features by combining or transforming the original features. The goal is to create a set of features that captures the essence of the original data in a lower-dimensional space. *Most of these are unsupervised learning.</p> </li> </ul>"},{"location":"machine_learning/#dimensionality-reduction-examples","title":"Dimensionality Reduction Examples","text":"<p>The most commonly used reduction technique - Principal Component Analysis (PCA)</p> <p>Also very popular, this is actually supervised learning, but is often compared with PCA - Linear Discriminate Analysis (LDA)</p>"},{"location":"machine_learning/#association-rule-learning","title":"Association Rule Learning","text":"<p>Also known as Association Rule Mining, is used to identify relationships, patterns and associations in datasets. This technique is commonly used for market basket analysis to better understand the relationship between different products. E.g., a store wants to find out the relationship between the sale of one product in relation to another, based on customer behavior. Like if a customer buys milk, then they may also buy bread, eggs, or butter.  </p> <p></p> <p>An example of this used for market basket analysis - Apriori Algorithm </p>"},{"location":"machine_learning/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Reinforcement learning is when an autonomous agent learns through trial and error with respect to its environment. Reinforcement learning seeks to maximize long-term reward over shot-term reward. It seeks to balance the discovery of new learning with the exploitation of existing learning. It is primarily geared towards sequential decision-making problems.\u200b\u200b</p> <p>The learning agent (with respect to a goal) gains information about its state from the surrounding environment. It uses this to determine the best action to take. After taking an action if there is either a reward or punishment signal from the environment it incorporates this into its understanding by either suppressing or encouraging that behavior in future.\u200b\u200b</p> <p>For the learning agent to discover optimum strategies need to experiment and try new behaviors. Through reward and punishment, it learns and retains new means of accomplishing a goal. It needs to still leverage known positive behaviors as the agent prioritizes achieving the largest cumulative reward. LLM's like ChatGPT have used human feedback as a form of reinforcement learning to fine tune models.</p> <p></p> <p>A video break-down, a bit easier with a animated example: Reinforcement Learning</p>"},{"location":"metrics/","title":"Model Metrics","text":"<p>Model metrics are an extremely important and useful tool when it comes measuring machine learning and AI models. These metrics give us a variety of methods to quantify various aspects of model performance in specific tasks. They enable us to make meaningful comparisons between different types of models so that we can pick the best solution for the job. </p> <p>In this section we will cover some of the most common performance metrics used in supervised learning for regression and classification tasks. This is by no means exhaustive, but is a solid base of understanding, which should be enough to start reading academic papers that focus on testing the performance of ML/AI models.</p>"},{"location":"metrics/#regression-metrics","title":"Regression Metrics","text":"<p>Regression seeks to gain an optimal result by minimizing an error term. Regression makes its prediction for each instance in a dataset and then compares this to the actual value (target output). The difference between these numbers is the random error for this instance. Regression takes the sum of all errors to quantify how accurate it was across the whole dataset (the smaller the better).</p> <p>\u200b A quick definition to know - 'absolute value' is the positive representation of any number (unsigned). This enables negative and positive numbers to be added together without them canceling out so you can preserve the overall size of an effect.</p>"},{"location":"metrics/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>Measures the average magnitude of prediction errors and is expressed in the same units as the target variable (e.g., mmHg for blood pressure, mg/dL for blood glucose). MAE is less sensitive to outliers compared to other metrics and its interpretability makes it a valuable tool for assessing model performance.\u200b</p>"},{"location":"metrics/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>Measures the average squared difference between predicted and actual values. This gives more weight to larger errors to have a disproportionate effect on this metric. If you optimized for MSE, your model would be particularly sensitive to larger errors, as they are weighted more heavily. </p>"},{"location":"metrics/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>The square-root of the MSE, this converts the MSE back to the units of the target variable making it easier to interpret. While RMSE, similar to MSE, it is sensitive to outliers, but is a balance between MSE and MAE.</p>"},{"location":"metrics/#mean-absolute-percentage-error-mape","title":"Mean Absolute Percentage Error (MAPE)","text":"<p>This metric takes each error as a percentage of the observed value. Then it takes the absolute mean of these percentages. This metric's main advantage is being scale agnostic (the units of measurement have no effect on it). This allows comparison of model performance across different datasets and ease of interoperability by stakeholders. To note, it can be undefined or disproportionately high when the actual values are close to zero.</p>"},{"location":"metrics/#r-squared","title":"R-Squared","text":"<p>Is a metric used to describe how much of the variance in the target variable is explained by the predictors. It's a value between 0 - 1, where 1 means a perfect  fit, which should always suspicious indicator of overfitting, there is no such thing as a perfect model. We can calculate R-Squared by the following steps:</p> <ul> <li> <p>Total Sum of Squares (TSS): We take the sum, of the square, of each observed value minus its mean. This captures the total variation in the target variable relative to its mean.</p> </li> <li> <p>Residual Sum of Squares (RSS): Nearly the same calculation as TSS but instead, we minus the predicted and observed values from each other. This measures the variation in the target that the model didn't capture.</p> </li> </ul> <p>R-Squared = (TSS-RSS) / TSS</p> <p>TSS - RSS captures how much variation in the target our model did capture. Then dividing it by TSS converts that to a proportion of total variance (0 - 1) making it agnostic to the units of measurement. \u200b</p>"},{"location":"metrics/#adjusted-r-squared","title":"Adjusted R-Squared","text":"<p>R-Squared does not consider the number of predictors in a model; it only measures the variance explained in the target. Adjusted R-Squared corrects for this by adjusting the R-Squared value based on the number of predictors and the sample size.\u200b</p> <p>It penalizes the addition of predictors that do not improve the model\u2019s explanatory power. If an added predictor does not increase the explained variance, Adjusted R-Squared will decrease, potentially indicating overfitting. This makes Adjusted R-Squared a more reliable metric for comparing models with different numbers of predictors.\u200b\u200b</p> <p>It is nearly always preferable to choose the simpler model with fewer predictors if it explains a comparable amount of variance in the target variable as another model with more predictors. This practice helps to reduce the risk of overfitting, lowers computational complexity, and potentially reduces the model's reliance on variables that do not significantly contribute to explaining the variance in the target variable.</p>"},{"location":"metrics/#classification-metrics","title":"Classification Metrics","text":"<p>Classification is a type of predictive modeling that aims to assign a label or category to each instance in a dataset. It seeks to determine the most likely class for each instance by analyzing the input features. The performance of a classification model is evaluated by comparing the predicted labels to the actual labels in the dataset.</p>"},{"location":"metrics/#four-types-of-predictions","title":"Four Types of Predictions","text":"<p>These are the four types of prediction a classification model can produce:</p> <p>True Positives (TP) - When the model and reality agree on a positive case.\u200b\u200b</p> <p>True Negatives (TN) - When the model and reality agree on a negative case.\u200b\u200b</p> <p>False Positives (FP) - When the model thinks an instance is a positive case, but it's actually negative (Type I error). \u200b\u200b</p> <p>False Negatives (FN) - When the model thinks an instance is a negative case, but it's actually positive (Type II error).</p> <p>The below image is a Confusion Matrix - its is a summary table commonly used to display these four measures in a friendly and convenient way ():</p> <p></p>"},{"location":"metrics/#accuracy","title":"Accuracy","text":"<p>Accuracy = (TP + TN) / (TP + TN + FP + FN) </p> <p>Accuracy is a simple measure that states the proportion of total cases that were correctly classified by the model.\u200b</p> <p>The inverse of accuracy (1 - accuracy) is called the 'classification error rate'</p>"},{"location":"metrics/#specificity-true-negative-rate","title":"Specificity (True Negative Rate)","text":"<p>Specificity = TN / (TN + FP)</p> <p>Specificity refers to the proportion of individuals without a disease who are correctly identified as negative by a test. This metric is essential when minimizing false positive results is crucial, such as in screening programs where unnecessary follow-up tests or treatments resulting from false alarms can be burdensome and cause undue anxiety for patients.</p>"},{"location":"metrics/#sensitivity-recall-true-positive-rate","title":"Sensitivity (Recall, True Positive Rate)","text":"<p>Sensitivity = TP / (TP + FN)</p> <p>Sensitivity measures the proportion of individuals with a disease who are correctly identified as positive by a test. For instance, if a test has a sensitivity of 0.7 and there are 100 patients with the disease, the test would accurately diagnose 70 of them. High sensitivity is crucial when failing to diagnose a patient with the disease (a false negative) carries severe consequences, even if it means accepting a higher rate of false positives.\u200b</p>"},{"location":"metrics/#precision","title":"Precision","text":"<p>Precision = TP / (TP + FP) </p> <p>Precision refers to the proportion of positive test results that truly represent the presence of the disease. For example, if a test has a precision of 0.85, it means that 85% of patients who test positive actually have the disease. Tests with high precision are crucial when the consequences of a false positive, such as unnecessary treatment or patient anxiety, are significant.\u200b</p>"},{"location":"metrics/#sensitivity-v-specificity-precision-trade-off","title":"Sensitivity v Specificity &amp; Precision Trade off","text":"<p>There's often a trade-off between sensitivity (the ability to correctly identify true positives, such as diseased patients) and both precision (the ability to avoid false positives, such as misdiagnosing healthy individuals) and sensitivity (the ability to identify healthy individuals).</p> <p>When optimizing for higher sensitivity in a model, often leads to decreases in precision and specificity where as the inverse is commonly true when optimizing for precision or specificity.</p> <p>This balance is especially crucial when dealing with conditions that are rare or have a high cost associated with misdiagnosis. Adjusting diagnostic thresholds or using a two-step approach with different tests can help optimize this balance based on the specific clinical context and the potential consequences of false results. The choice of diagnostic tools and strategies should carefully consider factors such as invasiveness, cost, and the potential impact on patient care.</p>"},{"location":"metrics/#f1-score","title":"F1 Score","text":"<p>F1 = 2\u00d7 ( (Precision * Sensitivity) / (Precision + Sensitivity) )</p> <p>F1 score is a metric that tries to provide a balanced measure of both recall and precision by using their harmonic mean. Where accuracy is agnostic to class imbalance F1 score is not. Where there is a notable class imbalance this can substantially influence the F1 score, often making it a much more informative metric compared to accuracy. F1 score also does not take true negatives into account which can be important when most cases are negative ones.\u200b</p>"},{"location":"metrics/#receiver-operating-characteristic-roc-curve","title":"Receiver Operating Characteristic (ROC) Curve","text":"<p>A graphical representation of a model's ability to distinguish between positive and negative classes. It plots Sensitivity (True Positive Rate) against the False Positive Rate (1 - Specificity).</p> <p></p>"},{"location":"metrics/#area-under-the-roc-curve-auc-roc","title":"Area Under the ROC Curve (AUC-ROC)","text":"<p>AUC-ROC numerically quantifies what is visually represented by the ROC curve. It takes the area under that curve and ranges from 0 - 1 where 1 is a perfect model. It is important to note that a AUC-ROC of 0.5, is equivalent to picking at random. A score below 0.5 would indicate a model performing worse than picking at random.</p>"},{"location":"modelling/","title":"Modelling","text":"<p>This section is an overview of some of the considerations taken when training AI/ML models.  From how data is formatted, scaled and split to ensure we can create the most relevant and valid models possible.</p>"},{"location":"modelling/#representing-the-world-in-data","title":"Representing the World in Data","text":"<p>To create computer models to help understand and make predictions of our physical world we need digital representations of it. Data, usually stored in the form of tables/dataframes,  making this information manageable, understandable and digestible for computer models.\u200b\u200b</p> <p>Each row of a table corresponds to one 'instance', a single entity that we are describing with data. If we are predicting house prices each row would be one house, or understanding causes of stroke each row would represent a person.\u200b\u200b</p> <p>The columns of a table are called 'features', each one a characteristic that describes something about the instances we analyzing. Features are usually but not always, either a number (number of rooms, total area, age, height, weight) or categorical (has central heating (y/n), internet type (dial-up, broadband, fiber, none), ethnicity, sex).</p> <p></p>"},{"location":"modelling/#data-preparation","title":"Data Preparation","text":"<p>Data seldom comes ready-to-use for computer models. It often requires uniform formatting across all features, we must address instances with missing information and determine if outliers (extreme values that stand out from the rest of the data) are retained or disposed of. For many models, categorical data must be encoded in numeric form, with techniques such as one-hot encoding.\u200b\u200b</p> <p>Another key issue is how the scale of numeric data can create biases. For instance, measuring length in millimeters results in larger numbers than if measured in centimeters, potentially leading models to overemphasize these features due to their larger numerical values. To prevent this, we scale data using techniques such as normalization, where all measurements are adjusted to fit between 0 and 1. This levels the playing field among different scales but can be thrown off by 'outliers,' which can stretch or compress the range in a way that doesn't reflect most of the data.\u200b\u200b</p> <p>Standardization is another method that's particularly handy when we need to maintain the natural spread of data, outliers included. It doesn't squash the data into a fixed range but repositions it so that the average value sits at zero, and we measure everything else by how much it differs from this average. Imagine grading on a curve: a score isn't just a number but tells you how it compares to the group average. This approach is less affected by outliers and helps in understanding the data's context and variability. So, what to use? It depends! Normalization is best when you want a neat, uniform scale without extreme values, while standardization is preferable for preserving relationships within the data, including those pesky outliers.</p>"},{"location":"modelling/#feature-vectors-feature-space-dimensionality","title":"Feature Vectors, Feature Space &amp; Dimensionality","text":"<p>Each 'instance' in a dataset is represented by a 'feature vector\u2019, containing all its characteristics. These vectors place each instance within a 'feature space\u2019. Imagine plotting age, height, and weight in a 3-dimensional space where each axis represents one of these features.</p> <p>However, as we add more features, the space expands beyond three dimensions, complicating visualization and analysis. This expansion can lead to the 'curse of dimensionality,' where too many features can make our data models less effective and harder to manage. \u200b</p> <p>You can measure distances in vector space. It enables algorithms to quantify the similarity between data points. For example,  Euclidean distance measures the 'straight-line' distance between two points in this space, which is intuitive in three dimensions but can extend to any number of dimensions. </p> <p></p>"},{"location":"modelling/#subsets-for-training-and-testing","title":"Subsets for Training and Testing","text":"<p>When training models, it's crucial to ensure they learn patterns and rules that apply not just to the data they're trained on but also to new, unseen data. To check for this during development, we use random sampling to split the data into separate sets. For tasks like classification, where the presence of each class might be uneven, 'stratified sampling' ensures that each split maintains the original proportion of classes.\u200b    \u200b</p> <p>A common split of data results in three sets:\u200b</p> <ul> <li> <p>Training Set: Contains most of the data, providing the model with ample information to learn from, including the target values needed for training.\u200b</p> </li> <li> <p>Validation Set: After training, the model is tested against this set where we measure its performance. This feedback helps us fine-tune the model through adjustments and retraining until we reach satisfactory performance level.\u200b</p> </li> <li> <p>Test Set: Used to evaluate the final version of the model against completely unseen data, offering the best estimate of how it will perform in the real world.</p> </li> </ul> <p></p>"},{"location":"modelling/#overfitting-and-underfitting","title":"Overfitting and Underfitting","text":"<p>In the quest for a model that performs well, we navigate between two common pitfalls: underfitting and overfitting.\u200b\u200b</p> <ul> <li> <p>Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. This usually means it won't perform well on the training set nor on new data, essentially failing to learn the crucial insights from the data it was provided.\u200b</p> </li> <li> <p>Overfitting is the opposite problem. Here, the model becomes so attuned to the training data that it learns its details and noise as if they were significant patterns. As a result, while it may perform exceptionally well on the training set, its performance drops significantly on new, unseen data because it has essentially memorized the training set, not learned generalizable patterns.    \u200b</p> </li> </ul> <p>There is also the issue of indirect overfitting. If we were to repeatedly adjust our model based on its performance on the test set, we would inadvertently start 'overfitting' to the test set. This would compromise the test set's role as an unbiased evaluator of the model's ability to generalize. Therefore, the validation set acts as a safeguard, providing a separate dataset for tweaking and improving the model.</p> <p></p>"},{"location":"modelling/#k-fold-cross-validation","title":"K-fold Cross Validation","text":"<p>K-fold cross-validation serves as an alternative to using fixed training and validation sets by partitioning the dataset into 'k' equal parts, or folds.\u200b</p> <p>In each iteration, one-fold is used as the validation set while the remaining folds collectively form the training set. This process repeats 'k' times, with each fold serving as the validation set exactly once.\u200b</p> <p>This technique ensures that all data is used for both training and validation, enhancing the model's learning and validation phases. However, it's important to maintain a separate test set, not involved in the cross-validation process.</p> <p></p>"},{"location":"modelling/#an-implementation-resource-for-aiml-and-data-science-in-health","title":"An implementation resource for AI/ML and Data Science in health:","text":"<p>Please make use of this excellent Te Whatu Ora resource focused on how to successfully implement some of the technology mentioned here in a health context - Health Data Science in Aotearoa New Zealand: A Practical Guide</p>"},{"location":"risk_management/","title":"Risk Management","text":""},{"location":"risk_management/#managing-and-quantifying-risks-in-ai","title":"Managing and Quantifying Risks in AI","text":"<p>The advent of AI in healthcare brings considerable benefits but also complex challenges that demand systematic risk management. There is a pressing need for robust frameworks to quantify and mitigate the inherent risks associated with AI deployment.</p> <p>Issues such as data bias must be preemptively addressed to ensure equitable outcomes. Privacy concerns and ethical considerations also require commitment to safeguard sensitive health information. By emphasizing the crucial elements of transparency, ethics, and oversight, such frameworks serve to guide healthcare entities in effectively managing risks.</p> <p>This ensures that the integration of AI into healthcare not only improves care but also maintains the trust and autonomy of patients, ultimately leading to safer outcomes and sustained public confidence in the use of AI technology in health.</p>"},{"location":"risk_management/#quantifying-risk-algorithm-charter-for-nz","title":"Quantifying Risk \u2013 Algorithm Charter for NZ","text":"<p>The Algorithm Charter for New Zealand provides a structured approach to quantifying AI/algorithm risk in the public sector by employing a risk matrix that assesses the likelihood of unintended consequences against their impact. This framework guides government agencies in applying AI thoughtfully, with focus a on potentially high associated risk for the public.</p> <p>By committing to transparency, safeguarding privacy, ethics, and human rights, and ensuring human oversight, the Charter can help healthcare agencies manage the use of AI, responsibly and ethically. This approach supports better decision-making, enhance public trust in how health-related data and AI is utilized, specifically addressing critical areas such as bias management and the protection of sensitive health information.</p> <p>Algorithm charter for Aotearoa New Zealand</p> <p></p>"},{"location":"risk_management/#fda-software-as-a-medical-device-samd","title":"FDA - Software as a Medical Device (SaMD)\u200b","text":"<p>The IMDRF's framework for SaMD provides an approach for categorizing and managing the risks associated with AI-driven medical software. By establishing a clear categorization based on the significance of the information provided by the SaMD and the healthcare context\u2014ranging from non-serious to critical\u2014this framework aids in identifying appropriate risk management strategies. \u200b</p> <p>It promotes AI in healthcare that is developed, deployed, and maintained with a strong emphasis on safety, efficacy, and security, addressing specific risks associated with the \u2018socio-technical  environment\u2019, system dependencies, and information security.</p> <p>Software as a Medical Device</p> <p></p>"}]}